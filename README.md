# STAT-241B-Final-Project
The paper we choose to base our project on is **Minimax Rate of Distribution Estimation on Unknown Submanifold under Adversarial Losses** by Rong Tang and Yun Yang. The first main point we will overview from the paper is generative models and the adversarial loss associated with them. In this paper, adversarial loss is defined as $d_{\mathcal{F}}(\mu_{1}, \mu_{2}) = \sup_{f \in \mathcal{F}} | \int_{\mathcal{X}} f(x) d\mu_{1} - \int_{\mathcal{X}} f(x) d\mu_{2} |$ where $\mu_{1}, \mu_{2} \in \mathcal{X}$ are two distributions in the data set $\mathcal{X}$ and $\mathcal{F}$ is some predefined set of functions. In this paper, they are concerned with functions in $C_{1}^{\gamma}(\mathbb{R}^{d})$, the unit ball of the $\gamma$-smooth HÃ¶lder class with $\gamma > 0$. Following this, we will overview some basic definitions for (sub)-manifolds, and introduce the partition of unity technique, which we will allow for manifolds that cannot be globally parameterized by a single chart to be used. After this setup, we will move into how to characterize distribution on a submanifold based on its action on smooth functions, with the partition of unity being crucial to showing this. We will also discuss the differences between the target distribution belonging to the class of smooth function versus being part of more general classes. Finally, we will overview a minimax upper bound for the rate of convergence of our defined adversarial loss, and show a procedure to achieve this rate by learning a mixture of generative models over a class. Overall, the first half of our project/presentation will be dedicated to helping someone without a rigorous background in differential topology to gain intuition behind the mathematical background before combining the main theorems in order to reach the minimax rate given by the paper. 
